{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "# from keras import backend as K\n",
    "# K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks import *\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 3743280\n"
     ]
    }
   ],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "print(\"Total Params:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'conv1_w'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m FRmodel\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss \u001b[38;5;241m=\u001b[39m triplet_loss, metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mload_weights_from_FaceNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFRmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sveta\\projects\\ComputerVision\\recognition\\facenet\\fr_utils.py:131\u001b[0m, in \u001b[0;36mload_weights_from_FaceNet\u001b[1;34m(FRmodel)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_weights_from_FaceNet\u001b[39m(FRmodel):\n\u001b[0;32m    129\u001b[0m     \u001b[39m# Load weights from csv files (which was exported from Openface torch model)\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     weights \u001b[39m=\u001b[39m WEIGHTS\n\u001b[1;32m--> 131\u001b[0m     weights_dict \u001b[39m=\u001b[39m load_weights()\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Set layer weights of the model\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m weights:\n",
      "File \u001b[1;32mc:\\Users\\Sveta\\projects\\ComputerVision\\recognition\\facenet\\fr_utils.py:152\u001b[0m, in \u001b[0;36mload_weights\u001b[1;34m()\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m WEIGHTS:\n\u001b[0;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mconv\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m name:\n\u001b[1;32m--> 152\u001b[0m         conv_w \u001b[39m=\u001b[39m genfromtxt(paths[name \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_w\u001b[39;49m\u001b[39m'\u001b[39;49m], delimiter\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    153\u001b[0m         conv_w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(conv_w, conv_shape[name])\n\u001b[0;32m    154\u001b[0m         conv_w \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(conv_w, (\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'conv1_w'"
     ]
    }
   ],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "img = cv2.imread('./images/sveta1.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "crop_img = img[y:y+h, x:x+w]\n",
    "res = cv2.resize(crop_img, dsize=(96, 96), interpolation=cv2.INTER_CUBIC)\n",
    "img = Image.fromarray(res, 'RGB')\n",
    "img.save('./images/sveta1_cropped.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"sveta serikova\"] = img_to_encoding('./images/sveta1_cropped.jpg', FRmodel)\n",
    "# database[\"borat\"] = img_to_encoding(\"images/sacha_cropped.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    encoding =  img_to_encoding(image_path, model)\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "    if dist < 0.7:\n",
    "        print(\"Это правда\" + str(identity) + \", проходи!\")\n",
    "        boolean = True\n",
    "    else:\n",
    "        print(\"Это не \" + str(identity) + \", оповещаю охрану \")\n",
    "        boolean = False        \n",
    "    return dist, boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imshow(\"Press SPACE to take a picture and ESC to close\", frame)\n",
    "    if not ret:\n",
    "        break\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    if k%256 == 27:\n",
    "        print(\"Camera Closed\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        img_name = \"webcam_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        res = cv2.imread(img_name)\n",
    "        img = Image.fromarray(res, 'RGB')\n",
    "        img.save('./images/webcam.jpg')    \n",
    "        print(\"{} has been added\".format(img_name))\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "img = cv2.imread('./images/webcam.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "crop_img = img[y:y+h, x:x+w]\n",
    "res = cv2.resize(crop_img, dsize=(96, 96), interpolation=cv2.INTER_CUBIC)\n",
    "img = Image.fromarray(res, 'RGB')\n",
    "img.save('./images/webcam.jpg')\n",
    "\n",
    "verify(\"./images/webcam.jpg\", 'sveta serikova' , database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если отказывает мне же, то увличить пороговое значение до 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('images/webcam.jpg')\n",
    "img2 = cv2.imread('images/san_cropped.jpg')\n",
    "print(\"############## IT'S A MATCH ###################\")\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax1.imshow(img1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "ax2.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc211b976e821c9e423ddd2e593ce830363ddc49505e602a6a63cbd2e71c3418"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
